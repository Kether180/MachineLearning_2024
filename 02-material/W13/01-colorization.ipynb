{
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": "py",
      "mimetype": "text/x-python",
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5,
  "cells": [
    {
      "cell_type": "code",
      "id": "css_setup",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        }
      },
      "source": [
        "import requests\n",
        "from IPython.core.display import HTML\n",
        "HTML(f\"\"\"\n",
        "<style>\n",
        "@import \"https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css\";\n",
        "</style>\n",
        "\"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "GETrfQfSU5cOZxPRQIm38",
      "metadata": {},
      "source": [
        "# Image colorization network\n",
        "<span style=\"color:red; font-size:20px;\">Notice that this exercise does not explicitly appear on the exam questions, however you may use it as part of the architecture.</span>\n",
        "The objective of this exercise is to implement a Unet CNN  model for image colorization. Solving the problem is an active field of research, so do not expect to obtain optimal results. Achieving perfect results is not the primary goal of this exercise. Instead, it is more important to reflect on the challenges and potential improvements in the various tasks. \n",
        "<article class=\"message is-danger\">\n",
        "  <div class=\"message-header\">Important</div>\n",
        "  <div class=\"message-body\">\n",
        "\n",
        "  Image generation/colorization is more memory and computation intensive than previous tasks and is also harder to train. While it is still possible to run the training for the current assignment on a CPU (an epoch will take around 20 min), it is recommended to use Google Colab, where an epoch takes around 6-7 minutes to complete. Only attempt this exercise, if you have plenty of time to complete the training.\n",
        "\n",
        "\n",
        "  </div>\n",
        "</article>\n",
        "The following cell loads the libraries.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "M3OVIIBOs1teAd7_jbQvf",
      "metadata": {},
      "source": [
        "### required libraries\n",
        "import os\n",
        "import torch\n",
        "import torchvision as tv\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Add additional import to ignore warnings\n",
        "import warnings\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "DQTA0pHZeyESZyY-dpE2c",
      "metadata": {},
      "source": [
        "## Data and data loader\n",
        "For this exercise you will use the Imagenette  dataset, which is a subset of Imagenet, containing 10 classes (tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute). \n",
        "The labels are not needed, as the task is to learn a mapping from grayscale to color images.\n",
        "The dataloader is already created for you below and it incrementally loads the dataset in batches to lessen memory requirements, returning the original and grayscale-converted images.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "JeQROvZreiHwiUqcspR-s",
      "metadata": {},
      "source": [
        "# Define the dataset class\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.transform = transform\n",
        "        # Recursively find all image files in root_dir\n",
        "        self.img_files = [p for p in self.root_dir.rglob('*') if p.is_file() and not p.name.startswith('.')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_files[idx]\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter(\"ignore\")\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Convert the transformed image to grayscale\n",
        "        image_gray = rgb_to_grayscale(image)\n",
        "\n",
        "        return image_gray, image\n",
        "    \n",
        "def rgb_to_grayscale(img_tensor):\n",
        "    # Define the weights for the RGB channels, which need to match the input tensor shape\n",
        "    r, g, b = img_tensor[0:1, :, :], img_tensor[1:2, :, :], img_tensor[2:3, :, :]\n",
        "    # Apply the weights\n",
        "    grayscale = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
        "    # Repeat the grayscale image across 3 channels to maintain the original tensor shape\n",
        "    return grayscale.repeat(3, 1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "xHK9ctjWnUa1sbxYMiUzc",
      "metadata": {},
      "source": [
        "The cell below configures the dataloader for both training and validation data. \n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "d2MZHiziMYSlGiEwqR4oX",
      "metadata": {},
      "source": [
        "# Define the transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Create the datasets\n",
        "train_dataset = CustomImageDataset(root_dir='imagenette2-160/train', transform=transform)\n",
        "val_dataset = CustomImageDataset(root_dir='imagenette2-160/val', transform=transform)\n",
        "\n",
        "# Create the DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4)\n",
        "\n",
        "for i,(gray,org) in enumerate(train_loader):\n",
        "    print(\"Grayscale images (the intensity is repeated across 3 channels to retain shape of original image)\",gray.shape)\n",
        "    print(\"Original data \",org.shape)\n",
        "    \n",
        "    if i>5:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "OgJy3B4tFZ4p-07udlhvx",
      "metadata": {},
      "source": [
        "<article class=\"message is-info\">\n",
        "  <div class=\"message-header\">Info</div>\n",
        "  <div class=\"message-body\">\n",
        "\n",
        "  If you struggle with running the training, the first thing to try is decreasing the batch size.\n",
        "\n",
        "\n",
        "  </div>\n",
        "</article>\n",
        "The cell below contains plotting functions for visualizing the original and grayscale images.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "kKgCtziSESy9nXXpYPIeG",
      "metadata": {},
      "source": [
        "def show_pairs(dataloader, n_pairs=4,model=None,device='cpu'):\n",
        "    for grayscale_images,original_images in dataloader:\n",
        "        # Show the first 'n_pairs' images from the batch\n",
        "        if model is None:\n",
        "            fig, axs = plt.subplots(n_pairs, 2, figsize=(6, 3 * n_pairs))\n",
        "\n",
        "            for i in range(n_pairs):\n",
        "                # Original Image\n",
        "                original_img = np.transpose(original_images[i].numpy(), (1, 2, 0))\n",
        "                axs[i, 0].imshow(original_img)\n",
        "                axs[i, 0].set_title('Original Image')\n",
        "                axs[i, 0].axis('off')\n",
        "\n",
        "                # Grayscale (or colorized) Image\n",
        "                grayscale_img = np.transpose(grayscale_images[i].numpy(), (1, 2, 0))\n",
        "                axs[i, 1].imshow(grayscale_img, cmap='gray' if grayscale_img.shape[2] == 1 else None)\n",
        "                axs[i, 1].set_title('Transformed Image')\n",
        "                axs[i, 1].axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        else:\n",
        "            fig, axs = plt.subplots(n_pairs, 3, figsize=(6, 3 * n_pairs))\n",
        "            model.to(device)\n",
        "            colored_images = model(grayscale_images).detach().numpy()\n",
        "            for i in range(n_pairs):\n",
        "\n",
        "                # Grayscale (or colorized) Image\n",
        "                transformed_img = np.transpose(grayscale_images[i].numpy(), (1, 2, 0))\n",
        "                axs[i, 0].imshow(transformed_img, cmap='gray' if transformed_img.shape[2] == 1 else None)\n",
        "                axs[i, 0].set_title('Transformed Image')\n",
        "                axs[i, 0].axis('off')\n",
        "                # Original Image\n",
        "                original_img = np.transpose(original_images[i].numpy(), (1, 2, 0))\n",
        "                axs[i, 1].imshow(original_img)\n",
        "                axs[i, 1].set_title('Original Image')\n",
        "                axs[i, 1].axis('off')\n",
        "\n",
        "                # Grayscale (or colorized) Image\n",
        "                colored_img = np.transpose(colored_images[i], (1, 2, 0))\n",
        "                axs[i, 2].imshow(colored_img)\n",
        "                axs[i, 2].set_title('NN Colorized Image')\n",
        "                axs[i, 2].axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show() \n",
        "        break  # Only show the first batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "LGHglTEYlY47h5aT4sDT7",
      "metadata": {},
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "4WQzHCaZ4YbsNBk5rjbW-",
      "metadata": {},
      "source": [
        "# Show a batch of images\n",
        "show_pairs(train_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "W_tcJ4OrI0NaxOvAcOSnf",
      "metadata": {},
      "source": [
        "<article class=\"message task\"><a class=\"anchor\" id=\"run\"></a>\n",
        "    <div class=\"message-header\">\n",
        "        <span>Task 1: Setup</span>\n",
        "        <span class=\"has-text-right\">\n",
        "          <i class=\"bi bi-stoplights easy\"></i>\n",
        "        </span>\n",
        "    </div>\n",
        "<div class=\"message-body\">\n",
        "\n",
        "\n",
        "Run all of the cells above. \n",
        "\n",
        "\n",
        "</div></article>\n",
        "\n",
        "The following sections cover the functions, classes, models you need to implement and train. \n",
        "## Unet\n",
        "[Unet](https://arxiv.org/abs/1505.04597)\n",
        " is the one of the most popular and well-performing architectures for image translation (segmentation, colorization, restoration, etc.) tasks. It consists of an encoder (compression), and a decoder (decompression). In addition, each encoder and decoder layer includes a skip-connection.\n",
        "This task guides you through implementation of a standard Unet architechture.\n",
        "**Unet building blocks**\n",
        "Each block in Unet handles a specific task for the network. \n",
        "### 1. Implementing the DoubleConv Block:\n",
        "A Double Convolution Block refers to a sequence of two consecutive convolutional layers within a neural network architecture. Double convolution block helps the model capture both low-level and high-level features through the two convolutional layers, while batch normalization and ReLU activation contribute to the stability and non-linearity of the network.\n",
        "<article class=\"message task\"><a class=\"anchor\" id=\"convblock\"></a>\n",
        "    <div class=\"message-header\">\n",
        "        <span>Task 2: Unet</span>\n",
        "        <span class=\"has-text-right\">\n",
        "          <i class=\"bi bi-code\"></i><i class=\"bi bi-stoplights medium\"></i>\n",
        "        </span>\n",
        "    </div>\n",
        "<div class=\"message-body\">\n",
        "\n",
        "\n",
        "Create a Double Convolution Block, known as `DoubleConv`\n",
        " in the U-Net architecture. It performs the following tasks sequentially:\n",
        "1. **Convolution**: Applies a 2D convolution to the input tensor with a certain number of input channels (`in_channels`\n",
        ") and a specified number of output channels ( or `out_channels`\n",
        " is not provided). The kernel size is set to 3x3, and padding is applied to maintain the spatial dimensions of the input.\n",
        "\n",
        "2. **Batch Normalization**: Normalize the output of the convolution layer with `nn.BatchNorm2d`\n",
        ".\n",
        "\n",
        "3. **ReLU Activation**: Apply a non-linear ReLU activation function (`nn.ReLU`\n",
        ")  in place to introduce non-linearity into the model.\n",
        "\n",
        "4. **Double up**: Repeat step 1-3 again: Another convolution is applied, followed by batch normalization and ReLU activation. Use sequential container `nn.Sequential`\n",
        " to stack the layers in the correct order for the forward pass.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "</div></article>\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "zGGRN1EqF5t0L15nYU2Ov",
      "metadata": {},
      "source": [
        "class DoubleConv(nn.Module):\n",
        "\"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "def __init__(self, in_channels, out_channels,kernel_size=3):\n",
        "super().__init__()\n",
        "self.double_conv = nn.Sequential(\n",
        "nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=(1,1)),\n",
        "...\n",
        ")\n",
        "\n",
        "def forward(self, x):\n",
        "return self.double_conv(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "N17YL9dglxryYQm6GJSM6",
      "metadata": {},
      "source": [
        "### 2.  Downsampling Block:\n",
        "The downsampling block:\n",
        "- Initialize a Max Pool layer with a kernel size of 2 to downsample the input tensor.\n",
        "- Uses the `DoubleConv`\n",
        " block to process the feature map post downsampling.\n",
        "\n",
        "### 3.  Upsampling Block:\n",
        "The upsampling block:\n",
        "- Bilinear interpolation/upsampling to the spatial dimensions of the feature maps by a scale factor of 2.\n",
        "- Concatenates the upsampled output with the corresponding feature map from the downsampling path (skip connection).\n",
        "- Uses the `DoubleConv`\n",
        " block to process the concatenated feature map.\n",
        "\n",
        "The downsampling and upsampling blocks play an important part of the of the Unet. However, to make the exercise more manageable, they are given in the code below.\n",
        "### 4 Assembling the U-Net Architecture:\n",
        "<article class=\"message task\"><a class=\"anchor\" id=\"create_unet\"></a>\n",
        "    <div class=\"message-header\">\n",
        "        <span>Task 3: Unet</span>\n",
        "        <span class=\"has-text-right\">\n",
        "          <i class=\"bi bi-code\"></i><i class=\"bi bi-stoplights medium\"></i>\n",
        "        </span>\n",
        "    </div>\n",
        "<div class=\"message-body\">\n",
        "\n",
        "\n",
        "Implement the entire U-Net architecture by combining the initial convolution (`DoubleConv`\n",
        "), the downsampling blocks (`Down`\n",
        "), the upsampling blocks (`Up`\n",
        ") in the network's forward pass. In the following sequence:\n",
        "1. **Initial Double Convolution**: The network begins with an initial double convolution applied to the input images.\n",
        "\n",
        "2. **Downsampling Path**: This is followed by a series of downsampling blocks that apply max pooling and double convolutions, reducing the spatial dimensions and increasing the depth of feature maps.\n",
        "\n",
        "3. **Bottleneck**: After the last downsampling block, the feature map reaches the bottleneck, which is the lowest resolution with the highest feature depth.\n",
        "\n",
        "4. **Upsampling Path**: Then, the network expands through a series of upsampling blocks that upsample the feature maps and combine them with the corresponding feature maps from the downsampling path through skip connections.\n",
        "\n",
        "5. **Final Convolution**: At the top of the upsampling path, the network applies a final convolution to map the deep feature maps to the desired number of output channels (here the 3 color channels).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "</div></article>\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "tJOw6VWasNdP2Ma4epVhO",
      "metadata": {},
      "source": [
        "#Downsampling\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "#Upsampling    \n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "        self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                                    diffY // 2, diffY - diffY // 2])\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=True,start_dim=16):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "        \n",
        "    # #initialize the different building blocks according to the following structure\n",
        "    #start: Dim\n",
        "    #Down: increase channels \n",
        "    #Down: \n",
        "    #Down: \n",
        "    #Down: \n",
        "    #Up: decrease channels\n",
        "    #up\n",
        "    #up\n",
        "    #up\n",
        "    #end\n",
        "    def forward(self, x):\n",
        "    # write code for forward pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "lvrQO9OzJYpd8PYIAkf2d",
      "metadata": {},
      "source": [
        "<article class=\"message task\"><a class=\"anchor\" id=\"train_epoch\"></a>\n",
        "    <div class=\"message-header\">\n",
        "        <span>Task 4: Train one epoch function</span>\n",
        "        <span class=\"has-text-right\">\n",
        "          <i class=\"bi bi-code\"></i><i class=\"bi bi-stoplights medium\"></i>\n",
        "        </span>\n",
        "    </div>\n",
        "<div class=\"message-body\">\n",
        "\n",
        "\n",
        "Implement the function `train_epoch`\n",
        " that performs a training loop for a neural network, (in this case Unet). The training follows the gradient descent optimization from the previous week, and consists of the following steps:\n",
        "1. **Set Model to Training Mode**: Activate training mode to enable layers like Dropout and BatchNorm to behave correctly during the training.\n",
        "\n",
        "2. **Initialize Running Loss**: Set up a variable to accumulate the loss over all batches within the epoch.\n",
        "\n",
        "3. **Iterate Over DataLoader**: Loop through batches of data provided by the DataLoader, which yields pairs of grayscale input images and original target images.\n",
        "\n",
        "4. **Transfer Data to Device**: Move input and target tensors to the configured computing device (CPU or GPU) to match the device where the model is located.\n",
        "\n",
        "5. **Execute Forward Pass**: Feed the input batch through the model to obtain predicted outputs.\n",
        "\n",
        "6. **Compute Loss**: Use the criterion to calculate the loss by comparing the model predictions with the actual target values.\n",
        "\n",
        "7. **Zero Gradients**: Clear old gradients; otherwise, they will accumulate with gradients of the current batch.\n",
        "\n",
        "8. **Perform Backward Pass**: Backpropagate the loss by computing its gradient with respect to each model parameter.\n",
        "\n",
        "9. **Update Model Parameters**: Adjust the model weights by performing a single optimization step using the gradients calculated during backpropagation.\n",
        "\n",
        "10. **Aggregate Loss**: Add the loss (multiplied by the batch size) to the running total to keep track of the total loss for the epoch.\n",
        "\n",
        "11. **Calculate Average Loss**: At the end of the epoch, divide the running loss by the number of samples in the dataset to get the average loss.\n",
        "\n",
        "12. **Print Epoch Loss**: Output the average loss for the epoch to monitor training progress.\n",
        "\n",
        "13. **Return Updated Model**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "</div></article>\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "u_K2OB2Ml9xsZ0VhNnhXQ",
      "metadata": {},
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"\n",
        "    Run one training epoch.\n",
        "\n",
        "    Parameters:\n",
        "    - model (torch.nn.Module): The neural network model to be trained.\n",
        "    - dataloader (torch.utils.data.DataLoader): DataLoader for the training dataset.\n",
        "    - criterion (torch.nn.modules.loss): The loss function.\n",
        "    - optimizer (torch.optim.Optimizer): The optimization algorithm.\n",
        "    - device (torch.device): The device to train on ('cuda' or 'cpu').\n",
        "\n",
        "    Returns:\n",
        "    - model (torch.nn.Module): The trained model.\n",
        "    \"\"\"\n",
        "    # #initialize the different building blocks according to the following structure"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4tD9Z2J7XE0lUX1v35OwT",
      "metadata": {},
      "source": [
        "If the `Unet`\n",
        " model class and `train_epoch`\n",
        " is implemented correctly the cells below can be executed for traning the Unet `model`\n",
        ".\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "avM1Dir0Wg2GOex6FTUuc",
      "metadata": {},
      "source": [
        "# Example usage:\n",
        "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = UNet(n_channels=3, n_classes=3,start_dim=64).to(device) ##\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "gEfc6YcsWii93thPV51Dh",
      "metadata": {},
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "oA03wuCZdaaIfbDJ7zrp4",
      "metadata": {},
      "source": [
        "# Call the function to run a single training epoch\n",
        "model = train_epoch(model, train_loader, criterion, optimizer, device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "o5Z97rkZOwwwCrjOpoG0e",
      "metadata": {},
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "2QV1t2juSIDQJVupOJ4n3",
      "metadata": {},
      "source": [
        "num_epochs = 10 \n",
        "for epoch in range(num_epochs):\n",
        "    train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    print(f'showing samples from epoch number {epoch}')\n",
        "    show_pairs(dataloader=train_loader,n_pairs=2,model=model)\n",
        "    show_pairs(dataloader=val_loader,n_pairs=2,model=model)\n",
        "    model.to(device)\n",
        "    print('------------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "S9it1eXWT1izOBtE0ZPS1",
      "metadata": {},
      "source": [
        "<article class=\"message task\"><a class=\"anchor\" id=\"reflections\"></a>\n",
        "    <div class=\"message-header\">\n",
        "        <span>Task 5: Reflection</span>\n",
        "        <span class=\"has-text-right\">\n",
        "          <i class=\"bi bi-lightbulb-fill\"></i><i class=\"bi bi-stoplights medium\"></i>\n",
        "        </span>\n",
        "    </div>\n",
        "<div class=\"message-body\">\n",
        "\n",
        "\n",
        "After completing a traning routine, consider the following questions:\n",
        "1. How does the U-Net architecture, particularly its use of skip connections, specifically aid in the task of image colorization? Reflect on how this architecture might perform differently compared to others you are familiar with for the same task.\n",
        "\n",
        "2. List atleast 3 significant limitations of the current model implementation? \n",
        "\n",
        "3. Provide suggestions for adressing each of the limitations:\n",
        "    - Keywords: Loss function, color spaces, hyperparameters, pretraining.\n",
        "\n",
        "\n",
        "4. Why is the task of colorization convinient from the perspective of data annotations?\n",
        "\n",
        "5. Could colorization serve as a good pretraining task?\n",
        "    - What must a model know about an image to color it?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "</div></article>\n",
        "\n",
        ""
      ]
    }
  ]
}