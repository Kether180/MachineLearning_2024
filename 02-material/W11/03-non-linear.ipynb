{
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": "py",
      "mimetype": "text/x-python",
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5,
  "cells": [
    {
      "cell_type": "code",
      "id": "css_setup",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        }
      },
      "source": [
        "import requests\n",
        "from IPython.core.display import HTML\n",
        "HTML(f\"\"\"\n",
        "<style>\n",
        "@import \"https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css\";\n",
        "</style>\n",
        "\"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "CoEgtWwHuMP3cpP24Ee4x",
      "metadata": {},
      "source": [
        "# Non-Linear Optimization: Multivariable Functions\n",
        "This exercise is about implementing non-linear optimization of multivariate functions. Non-linear optimization requires:\n",
        "- Compute partial derivatives.\n",
        "- Calculating forward and backward passes of the function.\n",
        "- Iteratively using the forward and backward passes.\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "8WUAJmrhskuoJj6gWckIB",
      "metadata": {},
      "source": [
        "# import packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "phaDXpVf4vTrZavUmVXQH",
      "metadata": {},
      "source": [
        "## Function Definition\n",
        "Define the function:\n",
        "\n",
        "$$ f(x, y) = e^{-x^2 - y^2} \\sin(x) \\cos(y) $$\n",
        "$f$ is interesting function because it includes an exponential decay and sinusoidal variation of both variables.\n",
        "## Class Implementation\n",
        "The cell below defines the class `ExpTrig`\n",
        " with the following methods:\n",
        "1. `forward`\n",
        " should return the function value of `f(x,y)`\n",
        ".\n",
        "2. `df_dx`\n",
        " should return the partial derivative of the function with respect to `x`\n",
        "\n",
        "3. `df_dy`\n",
        " should return the partial derivative of the function with respect to `y`\n",
        "\n",
        "4. `backward`\n",
        " should return the gradient of `f(x,y)`\n",
        " as a tuple `(df_dx, df_dy)`\n",
        ". \n",
        "5. `display_function`\n",
        " makes a figure of the function defined in `forward`\n",
        ".\n",
        "\n",
        "Your task will be to implement these in the following steps.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "njapBCgyYIMiSVuvLxst8",
      "metadata": {},
      "source": [
        "class ExpTrig:\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "        x: x-values (Can be single float or 1D array)\n",
        "        y: y-values (Can be single float or 1D array)\n",
        "\n",
        "        Returns:\n",
        "        The function values of f (size-like x and y)\n",
        "        \"\"\"\n",
        "        # Write the code here\n",
        "\n",
        "    def df_dx(self, x,y):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "        x: x-values (Can be single float or 1D numpy array)\n",
        "        y: y-values (Can be single float or 1D numpy array)\n",
        "\n",
        "        Returns:\n",
        "        The partial derivative of f with respect to x (size-like x and y)\n",
        "        \"\"\"\n",
        "        # Write the code here\n",
        "\n",
        "    def df_dy(self, x,y):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "        x: x-values (Can be single float or 1D numpy array)\n",
        "        y: y-values (Can be single float or 1D numpy array)\n",
        "\n",
        "        Returns:\n",
        "        The partial derivative of f with respect to y (size-like x and y)\n",
        "        \"\"\"\n",
        "        # Write the code here\n",
        "\n",
        "    def backward(self, x, y):\n",
        "        \"\"\"\n",
        "        args:\n",
        "        x: x-values\n",
        "        y: y-values\n",
        "        Returns:\n",
        "        Patial derivatives of the function (i.e. the gradient) as a tuple\n",
        "        \"\"\"\n",
        "        # Write the code here\n",
        "\n",
        "    def display_function(self):\n",
        "        \n",
        "        x = np.linspace(-2, 2, 400)\n",
        "        y = np.linspace(-2, 2, 400)\n",
        "        x, y = np.meshgrid(x, y)\n",
        "\n",
        "        z = self.forward(x,y)\n",
        "\n",
        "        # Create a 3D plot  \n",
        "        fig = plt.figure(figsize=(10, 8))\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        ax.plot_surface(x, y, z, cmap='viridis', alpha=0.8)\n",
        "        ax.set_xlabel('X axis')\n",
        "        ax.set_ylabel('Y axis')\n",
        "        ax.set_zlabel('f(x, y)')\n",
        "        ax.set_title('Surface of f(x, y) = e^{-x^2 - y^2} sin(x) cos(y)')\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "KFwGDF47ihFMXrbz4BuuN",
      "metadata": {},
      "source": [
        "<article class=\"message task\"><a class=\"anchor\" id=\"forward\"></a>\n",
        "    <div class=\"message-header\">\n",
        "        <span>Task 1: Forward pass</span>\n",
        "        <span class=\"has-text-right\">\n",
        "          <i class=\"bi bi-code\"></i><i class=\"bi bi-stoplights easy\"></i>\n",
        "        </span>\n",
        "    </div>\n",
        "<div class=\"message-body\">\n",
        "\n",
        "\n",
        "Implement the  `forward`\n",
        " function in the class above to evaluate the function `f(x,y)`\n",
        ".\n",
        "\n",
        "\n",
        "</div></article>\n",
        "\n",
        "<article class=\"message task\"><a class=\"anchor\" id=\"testing\"></a>\n",
        "    <div class=\"message-header\">\n",
        "        <span>Task 2: Testing the function</span>\n",
        "        <span class=\"has-text-right\">\n",
        "          <i class=\"bi bi-stoplights easy\"></i>\n",
        "        </span>\n",
        "    </div>\n",
        "<div class=\"message-body\">\n",
        "\n",
        "\n",
        "Run the code below to visualize the function.\n",
        "\n",
        "\n",
        "</div></article>\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "H9kLjXe23hGuuLUdw8izc",
      "metadata": {},
      "source": [
        "# Grid of x, y points\n",
        "f = ExpTrig()\n",
        "\n",
        "f.display_function()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "pJNCF-dj7-rxMxfM5dd37",
      "metadata": {},
      "source": [
        "## Partial Derivatives\n",
        "<article class=\"message task\"><a class=\"anchor\" id=\"partial\"></a>\n",
        "    <div class=\"message-header\">\n",
        "        <span>Task 3: Backward pass</span>\n",
        "        <span class=\"has-text-right\">\n",
        "          <i class=\"bi bi-stoplights medium\"></i>\n",
        "        </span>\n",
        "    </div>\n",
        "<div class=\"message-body\">\n",
        "\n",
        "\n",
        "1. On a piece of paper, find the partial derivatives of the function `f(x,y)`\n",
        "  with respect to `x`\n",
        " and `y`\n",
        " ( i.e. $\\frac{\\partial f}{\\partial x}$ and $\\frac{\\partial f}{\\partial y}$):\n",
        "\n",
        "2. Implement the functions `df_dx`\n",
        " and `df_dy`\n",
        " in the `ExpTrig`\n",
        " class, which return the values of the partial derivatives for a given `x`\n",
        " and `y`\n",
        ".\n",
        "\n",
        "\n",
        "3. Implement the `backward`\n",
        " pass method in the `ExpTrig`\n",
        " class, so that it returns the gradient evaluated in x, y.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "</div></article>\n",
        "\n",
        "## Optimization Method\n",
        "Basic gradient descent minimizes the function by iteratively adjusting variables in the opposite direction of the gradient:  \n",
        "$$ x_{t+1} = x_{t} -\\nabla_x f(x)_{t} \\lambda,$$\n",
        "\n",
        "where $\\lambda$ is the learning rate and $t$ refers to the interation step.\n",
        "In the following steps you will implement gradient descent of the function $f(x,y)$, as implemented in the class `ExpTrig`\n",
        ".\n",
        "<article class=\"message task\"><a class=\"anchor\" id=\"partial2\"></a>\n",
        "    <div class=\"message-header\">\n",
        "        <span>Task 4: Gradient descent</span>\n",
        "        <span class=\"has-text-right\">\n",
        "          <i class=\"bi bi-code\"></i><i class=\"bi bi-stoplights medium\"></i>\n",
        "        </span>\n",
        "    </div>\n",
        "<div class=\"message-body\">\n",
        "\n",
        "\n",
        "Implementing the `optimize_function`\n",
        " involves a series of steps that must be carried out both before and within the function itself. Here's a step-by-step guide:\n",
        "1. **Initialize Coordinates**: Start by initializing `x`\n",
        " and `y`\n",
        " with `start_x`\n",
        " and `start_y`\n",
        ", respectively.\n",
        "\n",
        "2. **Initialize History Lists**: Create two lists, `x_all`\n",
        " and `y_all`\n",
        ", to record the history of coordinates visited during the optimization.\n",
        "\n",
        "3. **Start Gradient Descent Loop**: Implement a loop that runs for the number of iterations specified. Within each iteration:\n",
        "    - Use the `backward`\n",
        " method of `func`\n",
        " to compute the gradients at the current `(x, y)`\n",
        ".\n",
        "    - Update `x`\n",
        " and `y`\n",
        " by taking a step in the direction opposite to the gradient, scaled by the `learning_rate`\n",
        ".\n",
        "\n",
        "\n",
        "4. **Logging (optional)**: Add a logging mechanism to print the current state of optimization. For example, after every 25 iterations, print the current iteration number, `x`\n",
        ", `y`\n",
        ", and the function value at this point.\n",
        "\n",
        "5. **Record History**: In each iteration, append the current values of `x`\n",
        " and `y`\n",
        " to `x_all`\n",
        " and `y_all`\n",
        " respectively.\n",
        "\n",
        "6. **Return the Result**: After completing all iterations, return `x_all`\n",
        " and `y_all`\n",
        ".\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "</div></article>\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "OuA6985fDR3usWgTTpkzH",
      "metadata": {},
      "source": [
        "def optimize_function(func, start_x, start_y, learning_rate, iterations):\n",
        "    \"\"\"\n",
        "    Optimize a given function using gradient descent.\n",
        "\n",
        "    Args:\n",
        "    - func (Function): A function object that must have 'forward' and 'backward' methods.\n",
        "                       The 'forward' method computes the function value at a given point (x, y),\n",
        "                       and the 'backward' method computes the gradient at that point.\n",
        "    - start_x (float): The starting x-coordinate for the optimization.\n",
        "    - start_y (float): The starting y-coordinate for the optimization.\n",
        "    - learning_rate (float): The step size for each iteration in the gradient descent.\n",
        "    - iterations (int): The total number of iterations for the optimization process.\n",
        "\n",
        "    Returns:\n",
        "    - x_all (list of float): List of all x-coordinates of the points visited during the optimization.\n",
        "    - y_all (list of float): List of all y-coordinates of the points visited during the optimization.\n",
        "\n",
        "    This function performs gradient descent on the provided function object. Starting from \n",
        "    (start_x, start_y), it iteratively moves in the direction opposite to the gradient, \n",
        "    with step sizes determined by the learning rate. The function's value and current \n",
        "    coordinates are printed every 25 iterations. The function returns the history of \n",
        "    coordinates visited during the optimization.\n",
        "    \"\"\"\n",
        "    # return ...\n",
        "    # Write the optimization rutine here\n",
        "\n",
        "# Example optimization\n",
        "x_arr, y_arr = optimize_function(ExpTrig(), start_x=-1.5, start_y=-1., learning_rate=0.2, iterations=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "l6Bxx6A37GpsTav41MJ8n",
      "metadata": {},
      "source": [
        "In the last snippet of code. We plot the optimization path of the `optimize_function`\n",
        " for the `ExpTrig`\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "Ku-pVjSRbaEji0MHwYBiU",
      "metadata": {},
      "source": [
        "x = np.linspace(-2, 2, 400)\n",
        "y = np.linspace(-2, 2, 400)\n",
        "x, y = np.meshgrid(x, y)\n",
        "z = ExpTrig().forward(x,y)\n",
        "\n",
        "# Assuming you have lists `x_values` and `y_values` containing the optimization path\n",
        "x_values = np.array(x_arr)\n",
        "y_values = np.array(y_arr)\n",
        "z_values = np.exp(-np.array(x_values)**2 - np.array(y_values)**2) * np.sin(x_values) * np.cos(y_values)\n",
        "\n",
        "# Plot the optimization path on the surface\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(x, y, z, cmap='viridis', alpha=0.6)\n",
        "ax.plot(x_values, y_values, z_values, marker='o', color='r', markersize=5, label='Optimization Path')\n",
        "ax.plot(x_values[-1], y_values[-1], z_values[-1], marker='x', color='b', markersize=10, label='Optimization Path')\n",
        "ax.set_xlabel('X axis')\n",
        "ax.set_ylabel('Y axis')\n",
        "ax.set_zlabel('f(x, y)')\n",
        "ax.set_title('Optimization Path on f(x, y) Surface')\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "X33l8UbDgwxytpxelJ5LE",
      "metadata": {},
      "source": [
        "<article class=\"message task\"><a class=\"anchor\" id=\"reflection\"></a>\n",
        "    <div class=\"message-header\">\n",
        "        <span>Task 5: Reflection</span>\n",
        "        <span class=\"has-text-right\">\n",
        "          <i class=\"bi bi-lightbulb-fill\"></i><i class=\"bi bi-stoplights medium\"></i>\n",
        "        </span>\n",
        "    </div>\n",
        "<div class=\"message-body\">\n",
        "\n",
        "\n",
        "Access the proficiency of the of the gradient descent algorithm\n",
        "1. Use the the following starting positions:\n",
        "\n",
        "$$x_{start}=1.5, y_{start}=1.5 $$\n",
        "\n",
        "\n",
        "$$x_{start}=-1.5,  y_{start}=-1.5$$\n",
        "\n",
        "\n",
        "$$x_{start}=-1.0, y_{start}=1.3$$\n",
        "\n",
        "\n",
        "$$x_{start}=1.2, y_{start}=-1.5$$\n",
        "\n",
        "\n",
        "- What do you observe?\n",
        "- Explain why the optimization function sometimes fails to find the $\\textbf{global}$ minimum?\n",
        "\n",
        "2. Do different learning rates (try $\\tau= \\{0.1,0.5,1.0\\}$ ) affect the result?\n",
        "3. List 2 different issues with a simple gradient descent optimization function.    - List a potential solution for each of the problems. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "</div></article>\n",
        "\n",
        ""
      ]
    }
  ]
}