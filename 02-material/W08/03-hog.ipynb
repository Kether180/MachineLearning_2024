{
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": "py",
      "mimetype": "text/x-python",
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5,
  "cells": [
    {
      "cell_type": "code",
      "id": "css_setup",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        }
      },
      "source": [
        "import requests\n",
        "from IPython.core.display import HTML\n",
        "HTML(f\"\"\"\n",
        "<style>\n",
        "@import \"https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css\";\n",
        "</style>\n",
        "\"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "FBfh0yp-yiK5Xs_EJqXSU",
      "metadata": {},
      "source": [
        "# Gradient image features: Histogram of Oriented Gradients (HoG) introduction\n",
        "This exercise is centered around extraction of gradient features from images. It consist of two parts:\n",
        "1. Construction of sobel (derivative) filters and convolving them with a sample image.\n",
        "2. Introduction and application of HoG features on a sample image.\n",
        "\n",
        "The following cell loads a sample image.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "K4IL_5SNKNQ9kq5EDGeXT",
      "metadata": {},
      "source": [
        "## load input image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "fn =  \"./data/people01.jpg\"\n",
        "image = plt.imread(fn)\n",
        "print(image.shape)\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(12, 8), sharex=True, sharey=True)\n",
        "\n",
        "ax1.axis('off')\n",
        "ax1.imshow(image, cmap=plt.cm.gray)\n",
        "ax1.set_title('Input image')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ZwC_SZRUPa7coEKRWt7S7",
      "metadata": {},
      "source": [
        "<article class=\"message task\"><a class=\"anchor\" id=\"gradient_features\"></a>\n",
        "    <div class=\"message-header\">\n",
        "        <span>Task 1: Apply sobel filters</span>\n",
        "        <span class=\"has-text-right\">\n",
        "          <i class=\"bi bi-code\"></i><i class=\"bi bi-stoplights medium\"></i>\n",
        "        </span>\n",
        "    </div>\n",
        "<div class=\"message-body\">\n",
        "\n",
        "\n",
        "The goal in this task is to construct derivative (sobel) filters in the x and y direction, and apply them by implementing a convolution algoritm.\n",
        "The following steps must be completed: \n",
        "1. Constuct the two gradient filters defined by the matrices below:\n",
        "\n",
        "\n",
        "$$\n",
        "sobel_x=\\left[\n",
        "\t\\begin{array}{r r r}\n",
        "\t\t-1 & 0 & 1 \\\\\n",
        "\t\t-2 & 0 & 2 \\\\\n",
        "\t\t-1 & 0 & 1\n",
        "\t\\end{array}\n",
        "\\right],\n",
        "sobel_y=\\left[\n",
        "\t\\begin{array}{r r r}\n",
        "\t\t-1 & -2 & -1 \\\\\n",
        "\t\t0 &  0 & 0 \\\\\n",
        "\t\t1 & 2 & 1\n",
        "\t\\end{array}\n",
        "\\right]\n",
        "$$\n",
        "2. (Optional) For the exercises last week, the `convolve`\n",
        " method from scipy was used to apply the filters. You are free to reuse the same method for this exercise. Optionally, this week we implement the method `convolve2d`\n",
        " from scratch. Below is a breakdown of the `convolve2d`\n",
        " function used for performing a 2D convolution operation on an image:\n",
        "    - Reverse the filter, flipping it both vertically and horizontally, to adhere to the convolution's mathematical definition.\n",
        "\n",
        "    - Output preparation, create an empty array, the same size as the image, to capture the convolution results.\n",
        "\n",
        "    - Add zero-padding around the image borders to ensure that the kernel properly processes the edges.\n",
        "\n",
        "    - Convolution process, iterate over each pixel. For each, apply the kernel, multiply its values with the image segment, and sum the results to assign a new value to the pixel.\n",
        "\n",
        "\n",
        "\n",
        "3. Apply the sobel filters using `convolve2d`\n",
        " (or `convolve`\n",
        " from scipy) on the sample image. Plot the results using the provided subplot template.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "</div></article>\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "Al1UnXGkhU_-Psk9oFGHN",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import convolve\n",
        "\n",
        "def rgb2gray(rgb):\n",
        "    \"\"\"Convert RGB image to grayscale\n",
        "       Parameters:\n",
        "        rgb : RGB image\n",
        "       Returns:\n",
        "        Grayscale image\n",
        "    \"\"\"\n",
        "    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n",
        "\n",
        "def convolve2d(image, kernel):\n",
        "    \"\"\"Perform 2D convolution on an image using a given kernel\n",
        "       Parameters:\n",
        "        image : 2D array\n",
        "        kernel : 2D kernel array\n",
        "       Returns:\n",
        "        Convolved 2D array\n",
        "    \"\"\"\n",
        "    write implementation here...\n",
        "\n",
        "\n",
        "# Convert the image to grayscale\n",
        "gray_image = rgb2gray(image)\n",
        "\n",
        "# Compute the gradients using the Sobel operator\n",
        "# Define the Sobel operator kernels.\n",
        "\n",
        "# Step 1: construct sobel filters \n",
        "...\n",
        "# Step 2: implement / finish\n",
        "...\n",
        "# Step 3: convolve the image with the filters\n",
        "...\n",
        "# Step 4:  Visualize the original image and gradients with the following subplot template\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(gray_image, cmap='gray'),\n",
        "plt.title('Original Grayscale Image')\n",
        "plt.xticks([]), plt.yticks([])  # Hides the tick values on X and Y axis\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(<place_gradient_x_image>, cmap='gray')\n",
        "plt.title('Gradient along X-axis')\n",
        "plt.xticks([]), plt.yticks([])  # Hides the tick values on X and Y axis\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(<place_gradient_y_image>, cmap='gray')\n",
        "plt.title('Gradient along Y-axis')\n",
        "plt.xticks([]), plt.yticks([])  # Hides the tick values on X and Y axis\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "o1oMPSpnxvzF34YxxpTdy",
      "metadata": {},
      "source": [
        "In the following section, you are reintroduced (see the classification lecture slides) to the [HOG descriptor](https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients)\n",
        ", shown how to calculate a HoG featues for an image, which includes a visualization of the HoG descriptor.\n",
        "## Overview of Histogram of Oriented Gradients (HOG) Algorithm\n",
        "The Histogram of Oriented Gradients (HOG) is a feature descriptor used in computer vision and image processing to detect objects. The process involves the following stages:\n",
        "**1. Preprocessing / Smoothing:**\n",
        "\n",
        "\n",
        "Initially, the image undergoes a global normalization application â€” a procedure that diminishes the effects of lighting discrepancies. A common technique involves gamma (power-law) compression, which can be achieved by calculating the square root or logarithm of each color channel. This stage helps in lessening the impact of local shadowing and highlights variations since the image texture strength is generally related to the local surface illumination.\n",
        "**2. Calculation of Image Gradients:**\n",
        "\n",
        "\n",
        "This stage focuses on computing the first-order image gradients. These gradients are crucial as they encapsulate information regarding contours, silhouettes, and textures. This process helps in maintaining resistance to different lighting conditions. The standard procedure is to either convert the image to grayscale or use the most prominent color channel. <article class=\"message is-info\">\n",
        "  <div class=\"message-header\">Info</div>\n",
        "  <div class=\"message-body\">\n",
        "\n",
        "  This step was implemented by us in the first part of the exercise.\n",
        "\n",
        "\n",
        "  </div>\n",
        "</article>\n",
        " \n",
        "**3. Creation of Cells and Calculation of Histograms:**\n",
        "\n",
        "\n",
        "The objective here is to create an encoding that is attuned to local image content, yet stable against minor alterations in posture or appearance. This method involves partitioning the image window into smaller spatial segments known as \"cells.\" Each cell undergoes a process to accumulate a local 1-D histogram that calculates gradient or edge orientations across all pixels within the cell. This histogram, known as the \"orientation histogram,\" categorizes the gradient angle range into several predetermined bins, with the gradient magnitudes of the cell's pixels contributing to the histogram's data.\n",
        "**4. Normalization Across Blocks:**\n",
        "\n",
        "\n",
        "The subsequent stage involves normalizing, which takes into account local arrays of cells, and performs contrast adjustments on their collective responses before the next phase. This step is pivotal for achieving invariance to lighting, shadowing, and edge contrast. It operates by gathering a metric of local histogram \"energy\" over localized cell groups termed \"blocks.\" This metric is then employed to normalize each cell within the block. Notably, while each cell might be common to multiple blocks, the normalizations are unique to each block, resulting in the cell's repeated appearance with distinct normalizations in the final output vector. Despite seeming redundant, this technique enhances performance. The descriptors obtained from these normalised blocks are what constitute the Histogram of Oriented Gradient (HOG) descriptors.\n",
        "**5. Compilation of Feature Vector:**\n",
        "\n",
        "\n",
        "The concluding step amalgamates the HOG descriptors from all block groupings, derived from a dense, overlapping grid spanning the detection window, into a unified feature vector. This vector is integral for the functionality of the window classifier.\n",
        "In essence, the HOG algorithm is a sophisticated method that converts image data into a format conducive to object detection, with considerable resistance to variations in lighting and posture.\n",
        "<article class=\"message task\"><a class=\"anchor\" id=\"descriptor\"></a>\n",
        "    <div class=\"message-header\">\n",
        "        <span>Task 2: Calculate the HOG descriptor</span>\n",
        "        <span class=\"has-text-right\">\n",
        "          <i class=\"bi bi-code\"></i><i class=\"bi bi-stoplights easy\"></i>\n",
        "        </span>\n",
        "    </div>\n",
        "<div class=\"message-body\">\n",
        "\n",
        "\n",
        "After block normalization, concatenate the resulting histograms into the final HOG feature descriptor (vector). \n",
        "Use the function [skimage.feature.hog](https://scikit-image.org/docs/stable/api/skimage.feature.html<elem-0>.feature.hog)\n",
        " to compute \n",
        "the feature descriptor vector from the resized image. Use the command:\n",
        "```python\n",
        "(fd, hog) = hog(\n",
        "image,\n",
        "orientations=bins,\n",
        "pixels_per_cell=(pixels_per_cell, pixels_per_cell),\n",
        "cells_per_block=(cells_per_block, cells_per_block),\n",
        "block_norm=\"L2\",\n",
        "visualize=True,\n",
        "feature_vector=False,\n",
        "channel_axis=-1)\n",
        "\n",
        "\n",
        "```\n",
        "The function `skimage.features.hog`\n",
        " also returns the feature descriptor vector (`fd`\n",
        "), in which its\n",
        "size is equal to the number of: Bins $\\times$ Block Columns\n",
        "$\\times$ Block Rows $\\times$ Cells in the Block.\n",
        "\n",
        "\n",
        "</div></article>\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "PAuYBGF9LeRSrVVfIMlB9",
      "metadata": {},
      "source": [
        "from skimage.feature import hog\n",
        "from skimage import data, exposure\n",
        "\n",
        "# load new image\n",
        "\n",
        "\n",
        "image1 = data.astronaut()[:270,100:320]\n",
        "print(image1.shape)\n",
        "\n",
        "feature_vector=True,\n",
        "bins = 8 \n",
        "pixels_per_cell = 16\n",
        "cells_per_block = 4 \n",
        "\n",
        "fd1,hog_feat1 = image # replace this with hog feature results. fd is the feature vector and hog_image is the visualization of the calculated gradients.\n",
        "\n",
        "# after implementation of the HoG function, run the following plot\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
        "\n",
        "ax1.axis('off')\n",
        "ax1.imshow(image1, cmap=plt.cm.gray)\n",
        "ax1.set_title('Input image')\n",
        "\n",
        "# Rescale histogram for better display\n",
        "hog_image_rescaled = exposure.rescale_intensity(hog_feat1, in_range=(0, 10))\n",
        "\n",
        "ax2.axis('off')\n",
        "ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray)\n",
        "ax2.set_title('Histogram of Oriented Gradients')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "nytUhQLCLeD-zXpCvelsO",
      "metadata": {},
      "source": [
        "<article class=\"message task\"><a class=\"anchor\" id=\"test\"></a>\n",
        "    <div class=\"message-header\">\n",
        "        <span>Task 3: Test the HOG descriptor</span>\n",
        "        <span class=\"has-text-right\">\n",
        "          <i class=\"bi bi-code\"></i><i class=\"bi bi-stoplights easy\"></i>\n",
        "        </span>\n",
        "    </div>\n",
        "<div class=\"message-body\">\n",
        "\n",
        "\n",
        "Test the HOG descriptor and function settings: \n",
        "Try changing:\n",
        "- The number of bins\n",
        "- The number of pixels per cell\n",
        "- The number of cells per block\n",
        "\n",
        "1. The number of cells per block does not change the visulization, why is that? what does it actually affect?\n",
        "2. How much does the features space reduce by doing HoG (compare to the default setting)?    - Follow-up (optional), how is the number of output features (i.e. shape of HoG feature vector) defined by the settings (number of bins, number of pixels per cell, and number of cells per block) <article class=\"message is-warning\">\n",
        "  <div class=\"message-header\">Tip</div>\n",
        "  <div class=\"message-body\">\n",
        "\n",
        "  Use the output shape of the feature vector to infer the relationship.\n",
        "\n",
        "\n",
        "  </div>\n",
        "</article>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "</div></article>\n",
        "\n",
        "In the next cell we load another image (of a cat) and run the same code as above\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "tZ3QmvygGm-3bxova7yKY",
      "metadata": {},
      "source": [
        "image2 = data.cat()[:270,:440:2]\n",
        "print(image2.shape)\n",
        "\n",
        "fd2,hog_image2 = image # replace this with hog feature results. fd is the feature vector and hog_image is the visualization of the calculated gradients.\n",
        "\n",
        "# after implementation of the HoG function, run the following plot\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
        "\n",
        "ax1.axis('off')\n",
        "ax1.imshow(image2, cmap=plt.cm.gray)\n",
        "ax1.set_title('Input image')\n",
        "\n",
        "# Rescale histogram for better display\n",
        "hog_image_rescaled2 = exposure.rescale_intensity(hog_feat2, in_range=(0, 10))\n",
        "\n",
        "ax2.axis('off')\n",
        "ax2.imshow(hog_image_rescaled2, cmap=plt.cm.gray)\n",
        "ax2.set_title('Histogram of Oriented Gradients')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "gUMVtx46tJDKgtuZp0D2j",
      "metadata": {},
      "source": [
        "<article class=\"message task\"><a class=\"anchor\" id=\"compare_images\"></a>\n",
        "    <div class=\"message-header\">\n",
        "        <span>Task 4: Use Hog Features</span>\n",
        "        <span class=\"has-text-right\">\n",
        "          <i class=\"bi bi-code\"></i><i class=\"bi bi-stoplights easy\"></i>\n",
        "        </span>\n",
        "    </div>\n",
        "<div class=\"message-body\">\n",
        "\n",
        "\n",
        "In this task compare the HoG features extracted from the two images. Specifically do the following:\n",
        "1. For different setting visually compare the hog features\n",
        "2. Calculate the normalized (use `np.linalg.norm`\n",
        " to normalize vectors) scalar product between the two feature vectors `fd1`\n",
        " and `fd2`\n",
        ".     - Was is the scalar product between two feacture vectors? (compare it to the value of 1 )\n",
        "    - Repeat the calculation for multiple HoG filter settings, notice a difference? \n",
        "\n",
        "\n",
        "\n",
        "#tip(Set `feature_vector`\n",
        "=True in `hog`\n",
        ")\n",
        "3. Based on the similarity, explain how HoG features can be used to classify different images of cat/humans.\n",
        "\n",
        "\n",
        "\n",
        "</div></article>\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "CMPuISjZMRmLOuAiur8IX",
      "metadata": {},
      "source": [
        "calculate the similarity here.."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "dto08oSyPfbrftTYJsWdk",
      "metadata": {},
      "source": [
        "<article class=\"message task\"><a class=\"anchor\" id=\"interpretation\"></a>\n",
        "    <div class=\"message-header\">\n",
        "        <span>Task 5: Interpretation/ Understanding questions</span>\n",
        "        <span class=\"has-text-right\">\n",
        "          <i class=\"bi bi-lightbulb-fill\"></i><i class=\"bi bi-stoplights medium\"></i>\n",
        "        </span>\n",
        "    </div>\n",
        "<div class=\"message-body\">\n",
        "\n",
        "\n",
        "Test your implementation by selecting\n",
        "multiple images available in our dataset (`peopleXY.jpg`\n",
        "). You\n",
        "have to compare (visually) the HOG features of people and non-people\n",
        "images. \n",
        "- What is visually needed for humans to identify objects visually, compared to hog features?\n",
        "- How would you apply HoG features in practice to classify different objects in a larger image with multiple people? \n",
        "\n",
        "\n",
        "\n",
        "</div></article>\n",
        "\n",
        "We will get back to that next week!\n",
        "The code in the cell below visualizes the HoG features:\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "4VkpK3GLFht0Fs6I1hZjA",
      "metadata": {},
      "source": [
        "# load new image\n",
        "fn =  \"./data/people01.jpg\"\n",
        "image = plt.imread(fn)\n",
        "print(image.shape)\n",
        "\n",
        "feature_vector=True,\n",
        "bins = 8 \n",
        "pixels_per_cell = 16\n",
        "cells_per_block = 4 \n",
        "\n",
        "fd,hog_image = image # replace this hog feature restults. fd is the feature vector and hog_image the visualization of the calculated gradients.\n",
        "\n",
        "print(f' HoG feature vector shape: {fd.shape}')\n",
        "print(f' HoG feature vector shape after vectorization: {fd.reshape(-1).shape}')\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4), sharex=True, sharey=True)\n",
        "\n",
        "ax1.axis('off')\n",
        "ax1.imshow(image, cmap=plt.cm.gray)\n",
        "ax1.set_title('Input image')\n",
        "\n",
        "# Rescale histogram for better display\n",
        "hog_image_rescaled = exposure.rescale_intensity(hog_feat, in_range=(0, 10))\n",
        "\n",
        "ax2.axis('off')\n",
        "ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray)\n",
        "ax2.set_title('Histogram of Oriented Gradients')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "_Mq2Qh8hkEfQRHZYU5ip3",
      "metadata": {},
      "source": [
        ""
      ]
    }
  ]
}